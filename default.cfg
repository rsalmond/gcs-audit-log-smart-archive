# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


[GCP]
# The project in which to set up audit logging and perform smart archive work.
PROJECT=CONFIGURE_ME


[RUNTIME]
# Number of worker threads. Each thread will process one row from BigQuery at a time, serially performing decision and action functions.
WORKERS=64

# Amount of work items to store. Mostly, items in this queue will be rows from either a BigQuery result set or a GCS API listing. More items will use more memory, but a larger work queue can improve performance if you see throughput stuttering.
WORK_QUEUE_SIZE=1000

# Perform a dry run. GCS rewrites will not be performed; all other functions, including writes to BigQuery tables, will be performed. If you have state you wish to preserve, back it up with a SELECT * materialized to a backup table.
# DRY_RUN=True

# Log level for the archiver. Default is INFO.
# LOG_LEVEL=DEBUG


[RULES]
# How many days between runs of the archiver (used to minimize BQ query work)
DAYS_BETWEEN_RUNS=1

# The cold storage class to use. Note that if you decide to use Coldline and data is already in Nearline, retrieval and possibly early deletion fees may apply.
COLD_STORAGE_CLASS=CONFIGURE_ME

# How many days without access after which the object should be moved to COLD_STORAGE_CLASS
COLD_THRESHOLD_DAYS=30

# How many days to check for recent access
WARM_THRESHOLD_DAYS=3

# How many accesses within WARM_THRESHOLD_DAYS after which the object should be moved to Standard storage. Note that this will include both creating and getting the object, so rewrites to a colder storage class will +1 this value.
WARM_THRESHOLD_ACCESSES=10

# Whether to record the original object created time in the objects_moved table. Causes an extra GET operation per object rewritten. Any value is true, default is false.
# RECORD_ORIGINAL_CREATE_TIME=True


[BIGQUERY]
# The dataset to use for storing audit logs and smart archive data.
DATASET_NAME=gcs_audit_logs

# Set the name of the temporary table for query results. A temporary table is 
# required because often the result set is too large to stream directly. You 
# *must* set this variable if you are running multiple instances of this 
# archiver using the same dataset for sharding or other reasons.
# TEMP_TABLE=smart_archiver_temp

# A catch-up table to use with object names and create times. This is useful for migrating objects which were created before audit logs were turned on. This data will be merged with the access logs so objects that are "hot" in this table will still be recognized as such.
# CATCHUP_TABLE=bucket_metadata

# How many rows to stream into BigQuery before starting a new stream.
# Default is 100.
# BATCH_WRITE_SIZE=200

# Project to use for running BQ jobs. State tables will also be written here.
# This is useful if you want to isolate resources dedicated to this program.
# Default is GCP.PROJECT
# JOB_PROJECT=

[AUDITLOGS]
# The name to use for the audit log sink. Used during install command.
SINK_NAME=smart_archiver_sink
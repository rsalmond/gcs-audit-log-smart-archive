# BASIC CONFIG
# The project in which to set up audit logging and perform smart archive work.
PROJECT=CONFIGURE_ME
# The dataset to use for storing audit logs and smart archive data.
DATASET_NAME=CONFIGURE_ME

# CLOUD FUNCTION SETTINGS
SCHEDULING_TOPIC=gcs_access_lifecycle
SCHEDULED_JOB_NAME=gcs_access_lifecycle
SCHEDULE_CRON=CONFIGURE_ME
FUNCTION_NAME=gcs_access_lifecycle
FUNCTION_MEMORY=512MB

# ARCHIVER RULES
# How many days between runs of the archiver (used to minimize BQ query work)
DAYS_BETWEEN_RUNS=1
# The cold storage class to use. Note that if you decide to use Coldline and data is already in Nearline, retrieval and possibly early deletion fees may apply.
COLD_STORAGE_CLASS=CONFIGURE_ME
# How many days without access after which the object should be moved to COLD_STORAGE_CLASS
COLD_THRESHOLD_DAYS=30
# How many days to check for recent access
WARM_THRESHOLD_DAYS=3
# How many accesses within WARM_THRESHOLD_DAYS after which the object should be moved to Standard storage
WARM_THRESHOLD_ACCESSES=6

# DETAILS
# How many rows to stream into BigQuery before starting a new stream.
BQ_BATCH_WRITE_SIZE=100
# Log level for the archiver.
LOG_LEVEL=DEBUG

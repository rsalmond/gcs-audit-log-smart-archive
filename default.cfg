# BASIC CONFIG
# The project in which to set up audit logging and perform smart archive work.
PROJECT=CONFIGURE_ME
# The dataset to use for storing audit logs and smart archive data.
DATASET_NAME=CONFIGURE_ME

# CLOUD FUNCTION SETTINGS
SCHEDULING_TOPIC=gcs_access_lifecycle
SCHEDULED_JOB_NAME=gcs_access_lifecycle
SCHEDULE_CRON=CONFIGURE_ME
FUNCTION_NAME=gcs_access_lifecycle
FUNCTION_MEMORY=512MB

# ARCHIVER RULES
# How many days between runs of the archiver (used to minimize BQ query work)
DAYS_BETWEEN_RUNS=1
# The cold storage class to use. Note that if you decide to use Coldline and data is already in Nearline, retrieval and possibly early deletion fees may apply.
COLD_STORAGE_CLASS=CONFIGURE_ME
# How many days without access after which the object should be moved to COLD_STORAGE_CLASS
COLD_THRESHOLD_DAYS=30
# How many days to check for recent access
WARM_THRESHOLD_DAYS=3
# How many accesses within WARM_THRESHOLD_DAYS after which the object should be moved to Standard storage. Note that this will include both creating and getting the object, so rewrites to a colder storage class will +1 this value.
WARM_THRESHOLD_ACCESSES=10
# Whether to record the original object created time in the objects_moved table. Causes an extra GET operation per object rewritten.
RECORD_ORIGINAL_CREATE_TIME=1
# Whether to record the original object created time in the objects_moved table. Causes an extra GET operation per object rewritten. Any value is true, default is false.
# RECORD_ORIGINAL_CREATE_TIME=True
# A catch-up table to use with object names and create times. This is useful for migrating objects which were created before audit logs were turned on. This data will be merged with the access logs so objects that are "hot" in this table will still be recognized as such.
# CATCHUP_TABLE=bucket_metadata

# DETAILS
# How many rows to stream into BigQuery before starting a new stream.
BQ_BATCH_WRITE_SIZE=100
# Log level for the archiver.
LOG_LEVEL=DEBUG
